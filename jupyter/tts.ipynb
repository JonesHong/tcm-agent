{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published: {'text': '\\n            畫面中，前方黑衣騎士才過一個大轉彎，突然右方一輛路邊停車的轎車開車門，\\n            騎士沒閃過，直接被吃車門擊落，連人帶車噴飛在車道上，落地前安全帽掉落，\\n            騎士還在地面上滑行滾了一圈才停下，地面上留下長長一道刮痕。\\n           '}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在目前儲存格或上一個儲存格中執行程式碼時，Kernel 已損毀。\n",
      "\u001b[1;31m請檢閱儲存格中的程式碼，找出失敗的可能原因。\n",
      "\u001b[1;31m如需詳細資訊，請按一下<a href='https://aka.ms/vscodeJupyterKernelCrash'>這裡</a>。\n",
      "\u001b[1;31m如需詳細資料，請檢視 Jupyter <a href='command:jupyter.viewOutput'>記錄</a>。"
     ]
    }
   ],
   "source": [
    "import redis\n",
    "import json\n",
    "\n",
    "class RedisChannel:\n",
    "    do_tts_service = \"do-tts-service\"\n",
    "    tts_done_service = \"tts-done-service\"\n",
    "    do_asr_service = \"do-asr-service\"\n",
    "    asr_done_service = \"asr-done-service\"\n",
    "    \n",
    "def publisher(channel, data):\n",
    "    message = json.dumps(data)\n",
    "    redis_client_pub.publish(channel, message)\n",
    "    print(f\"Published: {data}\")\n",
    "redis_client_pub = redis.Redis(host='localhost', port=51201)\n",
    "# redis_client_pub = redis.Redis(host='localhost', port=6379)\n",
    "\n",
    "# # publisher(RedisChannel.do_tts_service, 3333)\n",
    "\n",
    "message = {\n",
    "    \"text\":\"\"\"\n",
    "            畫面中，前方黑衣騎士才過一個大轉彎，突然右方一輛路邊停車的轎車開車門，\n",
    "            騎士沒閃過，直接被吃車門擊落，連人帶車噴飛在車道上，落地前安全帽掉落，\n",
    "            騎士還在地面上滑行滾了一圈才停下，地面上留下長長一道刮痕。\n",
    "           \"\"\"}\n",
    "publisher(RedisChannel.do_tts_service, message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -*- coding: utf-8 -*-\n",
    "\n",
    "# import os\n",
    "# import sys\n",
    "\n",
    "# project_path = os.getcwd()\n",
    "# print(project_path)\n",
    "# # 获取当前脚本的绝对路径\n",
    "# # current_script_path = os.path.abspath(__file__)\n",
    "# current_script_path = os.path.abspath(f\"{project_path}/services/tts.service.py\")\n",
    "# # 获取当前脚本的目录路径\n",
    "# current_directory = os.path.dirname(current_script_path)\n",
    "# # 回退到 asr 目录（当前目录的父目录的父目录）\n",
    "# asr_directory = os.path.dirname(current_directory)\n",
    "# # 构建目标目录路径\n",
    "# packages_path = os.path.join(asr_directory, 'packages')\n",
    "# # 添加到 sys.path\n",
    "# sys.path.append(packages_path)\n",
    "# # 打印添加的路径以确认\n",
    "# print(\"Added to sys.path:\", packages_path)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "project_path = os.getcwd()\n",
    "print(project_path)\n",
    "sys.path.append(f'{project_path}/packages')\n",
    "sys.path.append(f'{project_path}/packages/vits/text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from vits.text import text_to_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "from vits.text import text_to_sequence\n",
    "import vits.commons as commons\n",
    "import vits.utils as utils\n",
    "from vits.models import SynthesizerTrn\n",
    "from torch import no_grad, LongTensor\n",
    "import torch\n",
    "\n",
    "import wave\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "class VitsService:\n",
    "    def __init__(self,hparams_file_path = \"./models/community/model_config.json\",checkpoint_path=\"./models/community/G_953000.pth\"):\n",
    "        self._hparams_file_path = hparams_file_path \n",
    "        self._checkpoint_path = checkpoint_path\n",
    "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.language_marks = {\n",
    "            \"Japanese\": \"\",\n",
    "            \"日本語\": \"[JA]\",\n",
    "            \"中文\": \"[ZH]\",\n",
    "            \"English\": \"[EN]\",\n",
    "            \"Mix\": \"\",\n",
    "            }\n",
    "        self.lang = ['日本語', '中文', 'English', 'Mix']\n",
    "        self.hps = utils.get_hparams_from_file(self._hparams_file_path)\n",
    "        self.set_model()\n",
    "\n",
    "    def set_model(self):\n",
    "        self.net_g = SynthesizerTrn(\n",
    "            len(self.hps.symbols),\n",
    "            self.hps.data.filter_length // 2 + 1,\n",
    "            self.hps.train.segment_size // self.hps.data.hop_length,\n",
    "            n_speakers=self.hps.data.n_speakers,\n",
    "            **self.hps.model).to(self.device)\n",
    "        _ = self.net_g.eval()\n",
    "\n",
    "        _ = utils.load_checkpoint(self._checkpoint_path, self.net_g, None)\n",
    "\n",
    "        self.speaker_ids = self.hps.speakers\n",
    "\n",
    "    def get_text(self,text, hps, is_symbol):\n",
    "        text_norm = text_to_sequence(text, hps.symbols, [] if is_symbol else hps.data.text_cleaners)\n",
    "        if hps.data.add_blank:\n",
    "            text_norm = commons.intersperse(text_norm, 0)\n",
    "        text_norm = LongTensor(text_norm)\n",
    "        return text_norm\n",
    "\n",
    "    def create_tts_fn(self):\n",
    "        def tts_fn(text, speaker, language, speed):\n",
    "            if language is not None:\n",
    "                text = self.language_marks[language] + text + self.language_marks[language]\n",
    "            speaker_id = self.speaker_ids[speaker]\n",
    "            stn_tst = self.get_text(text, self.hps, False)\n",
    "            with no_grad():\n",
    "                x_tst = stn_tst.unsqueeze(0).to(self.device)\n",
    "                x_tst_lengths = LongTensor([stn_tst.size(0)]).to(self.device)\n",
    "                sid = LongTensor([speaker_id]).to(self.device)\n",
    "                audio = self.net_g.infer(x_tst, x_tst_lengths, sid=sid, noise_scale=.667, noise_scale_w=0.8,\n",
    "                                    length_scale=1.0 / speed)[0][0, 0].data.cpu().float().numpy()\n",
    "            del stn_tst, x_tst, x_tst_lengths, sid\n",
    "            return \"Success\", (self.hps.data.sampling_rate, audio)\n",
    "\n",
    "        return tts_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import wave\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tts = VitsService(\n",
    "    hparams_file_path=\"./models/YunzeNeural/config.json\",\n",
    "    checkpoint_path=\"./models/YunzeNeural/G_latest.pth\"\n",
    ")\n",
    "tts_fn = tts.create_tts_fn()\n",
    "speaker = 'YunzeNeural'\n",
    "language='中文'\n",
    "speed = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_voice_array(tts_fn, speaker ,text: str) -> list:\n",
    "    text = text.replace('\\\\n','').strip().replace('\\n','')\n",
    "    symbol_remov = [\"，\", \"、\", \"。\",\"：\", \"）\", \"（\", \"？\", \":\", \")\", \"(\",\"「\", \"」\", \"！\"]\n",
    "    non_symbol_text = text\n",
    "    for i in symbol_remov:\n",
    "        non_symbol_text = non_symbol_text.replace(i, \"@\")\n",
    "    current_index = 0\n",
    "    max_len = 30\n",
    "    numpy_voice_array = []\n",
    "    if max_len >= len(text):\n",
    "\n",
    "            _, output = tts_fn(text=text,speaker=speaker,language=language,speed=float(speed))\n",
    "            sr = output[0]\n",
    "            numpy_voice_array = output[1].tolist()\n",
    "    else:\n",
    "        while True:\n",
    "            temp_text = non_symbol_text[current_index:current_index + max_len]\n",
    "            split_index = [pos for pos, char in enumerate(temp_text) if char == '@']\n",
    "            if len(split_index) == 0:\n",
    "                target_index = current_index + max_len + 1\n",
    "            else:\n",
    "                target_index = split_index[-1] + current_index + 1\n",
    "            used_text = text[current_index:target_index]\n",
    "            # print(\"Used Text: \", used_text)\n",
    "            current_index = target_index\n",
    "            _, output = tts_fn(text=used_text,speaker=speaker,language=language,speed=float(speed))\n",
    "            sr = output[0]\n",
    "            voice_list = output[1].tolist()\n",
    "            # print(len(voice_list))\n",
    "            if len(voice_list) < 20000:\n",
    "                numpy_voice_array += voice_list\n",
    "            else:\n",
    "                numpy_voice_array += output[1].tolist()[300:-8500]\n",
    "            if current_index >= len(text)-1:\n",
    "                break\n",
    "    return numpy_voice_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mp3(tts_fn, speaker, text, filename):\n",
    "    text = text.replace('\\\\n','').strip().replace('\\n','')\n",
    "    symbol_remov = [\"，\", \"、\", \"。\",\"：\", \"）\", \"（\", \"？\", \":\", \")\", \"(\",\"「\", \"」\", \"！\"]\n",
    "    non_symbol_text = text\n",
    "    for i in symbol_remov:\n",
    "        non_symbol_text = non_symbol_text.replace(i, \"@\")\n",
    "    current_index = 0\n",
    "    max_len = 30\n",
    "    numpy_voice_array = []\n",
    "    if max_len >= len(text):\n",
    "\n",
    "            _, output = tts_fn(text=text,speaker=speaker,language=language,speed=float(speed))\n",
    "            sr = output[0]\n",
    "            numpy_voice_array = output[1].tolist()\n",
    "    else:\n",
    "        while True:\n",
    "            temp_text = non_symbol_text[current_index:current_index + max_len]\n",
    "            split_index = [pos for pos, char in enumerate(temp_text) if char == '@']\n",
    "            if len(split_index) == 0:\n",
    "                target_index = current_index + max_len + 1\n",
    "            else:\n",
    "                target_index = split_index[-1] + current_index + 1\n",
    "            used_text = text[current_index:target_index]\n",
    "            # print(\"Used Text: \", used_text)\n",
    "            current_index = target_index\n",
    "            _, output = tts_fn(text=used_text,speaker=speaker,language=language,speed=float(speed))\n",
    "            sr = output[0]\n",
    "            voice_list = output[1].tolist()\n",
    "            # print(len(voice_list))\n",
    "            if len(voice_list) < 20000:\n",
    "                numpy_voice_array += voice_list\n",
    "            else:\n",
    "                numpy_voice_array += output[1].tolist()[300:-8500]\n",
    "            if current_index >= len(text)-1:\n",
    "                break\n",
    "                \n",
    "    numpy_voice_array2 = np.int16(np.array(numpy_voice_array)* 32767)\n",
    "    if not filename:\n",
    "        file_str = datetime.strftime(datetime.now(), '%Y-%m-%d-%H-%M-%S') + \".wav\"\n",
    "    else:\n",
    "        file_str = filename + \".wav\"\n",
    "\n",
    "    with wave.open(file_str, \"wb\") as wf:\n",
    "        wf.setnchannels(1)  # 设置声道数\n",
    "        wf.setsampwidth(2)  # 设置样本宽度（字节数）\n",
    "        wf.setframerate(sr)  # 设置采样率\n",
    "        wf.writeframes(numpy_voice_array2.tobytes())  # 写入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "1. 你的睡眠如何? 是否一覺到天亮? 是否每天定時會醒? 如果會醒, 是幾點會醒? 是否多夢? \n",
    "\n",
    "2。 你感覺餓嗎？有欲望想吃什麼特別的食物或是喜愛什麼味道的食物？或是不餓，完全沒有胃口。\n",
    "\n",
    "3。 你便秘嗎？每天有大便嗎？大便顏色是什麼？是下利嗎？很臭還是無味？\n",
    "\n",
    "4。 你的小便是什麼顏色？頻尿嗎？還是小不出來？還是沒有尿意？平均一天幾次？\n",
    "\n",
    "5。 你很渴嗎？如渴，最想喝什麼溫度的水？如不渴，時常會忘記喝水嗎？還是再怎麼喝也不能止渴呢？\n",
    "\n",
    "6。 你平時覺得身體很熱還是很冷？手腳冰冷嗎？\n",
    "\n",
    "7。 你容易出汗嗎？會半夜盜汗嗎？會時常流汗不止嗎？還是不出汗的身體呢？\n",
    "\n",
    "8。 精神好嗎？還是一直疲憊中？早上起床時，是精神奕奕呢？還是無法起床呢？精神能夠集中嗎？\n",
    "\n",
    "9。 你性功能好嗎？\n",
    "\n",
    "10。無論妳有無月經，都要詳細說明妳的月經情形，是延後還是每次都提前呢？痛不痛呢？生過小孩嗎？\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_mp3(tts_fn, speaker, text, \"testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "project_path = os.getcwd()\n",
    "print(project_path)\n",
    "sys.path.append(f'{project_path}/packages')\n",
    "# 获取当前脚本的绝对路径\n",
    "# current_script_path = os.path.abspath(__file__)\n",
    "# # 获取当前脚本的目录路径\n",
    "# current_directory = os.path.dirname(current_script_path)\n",
    "# # 回退到 asr 目录（当前目录的父目录的父目录）\n",
    "# asr_directory = os.path.dirname(current_directory)\n",
    "# # 构建目标目录路径\n",
    "# packages_path = os.path.join(asr_directory, 'packages')\n",
    "# # 添加到 sys.path\n",
    "# sys.path.append(packages_path)\n",
    "\n",
    "from vits.text import text_to_sequence\n",
    "import vits.commons as commons\n",
    "import vits.utils as utils\n",
    "from vits.models import SynthesizerTrn\n",
    "from torch import no_grad, LongTensor\n",
    "import torch\n",
    "\n",
    "import wave\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "class VitsService:\n",
    "    def __init__(self,hparams_file_path = \"./models/community/config.json\",checkpoint_path=\"./models/community/G_latest.pth\"):\n",
    "        self._hparams_file_path = hparams_file_path \n",
    "        self._checkpoint_path = checkpoint_path\n",
    "        \n",
    "        print(hparams_file_path)\n",
    "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.language_marks = {\n",
    "            \"Japanese\": \"\",\n",
    "            \"日本語\": \"[JA]\",\n",
    "            \"中文\": \"[ZH]\",\n",
    "            \"English\": \"[EN]\",\n",
    "            \"Mix\": \"\",\n",
    "            }\n",
    "        self.lang = ['日本語', '中文', 'English', 'Mix']\n",
    "                \n",
    "        self.tts_fn = self.create_tts_fn()\n",
    "        self.hps = utils.get_hparams_from_file(self._hparams_file_path)\n",
    "        self.set_model()\n",
    "\n",
    "    def set_model(self):\n",
    "        self.net_g = SynthesizerTrn(\n",
    "            len(self.hps.symbols),\n",
    "            self.hps.data.filter_length // 2 + 1,\n",
    "            self.hps.train.segment_size // self.hps.data.hop_length,\n",
    "            n_speakers=self.hps.data.n_speakers,\n",
    "            **self.hps.model).to(self.device)\n",
    "        _ = self.net_g.eval()\n",
    "\n",
    "        _ = utils.load_checkpoint(self._checkpoint_path, self.net_g, None)\n",
    "\n",
    "        self.speaker_ids = self.hps.speakers\n",
    "\n",
    "    def get_text(self,text, hps, is_symbol):\n",
    "        text_norm = text_to_sequence(text, hps.symbols, [] if is_symbol else hps.data.text_cleaners)\n",
    "        if hps.data.add_blank:\n",
    "            text_norm = commons.intersperse(text_norm, 0)\n",
    "        text_norm = LongTensor(text_norm)\n",
    "        return text_norm\n",
    "\n",
    "    def create_tts_fn(self):\n",
    "        def tts_fn(text, speaker, language, speed):\n",
    "            if language is not None:\n",
    "                text = self.language_marks[language] + text + self.language_marks[language]\n",
    "            speaker_id = self.speaker_ids[speaker]\n",
    "            stn_tst = self.get_text(text, self.hps, False)\n",
    "            with no_grad():\n",
    "                x_tst = stn_tst.unsqueeze(0).to(self.device)\n",
    "                x_tst_lengths = LongTensor([stn_tst.size(0)]).to(self.device)\n",
    "                sid = LongTensor([speaker_id]).to(self.device)\n",
    "                audio = self.net_g.infer(x_tst, x_tst_lengths, sid=sid, noise_scale=.667, noise_scale_w=0.8,\n",
    "                                    length_scale=1.0 / speed)[0][0, 0].data.cpu().float().numpy()\n",
    "            del stn_tst, x_tst, x_tst_lengths, sid\n",
    "            return \"Success\", (self.hps.data.sampling_rate, audio)\n",
    "\n",
    "        return tts_fn\n",
    "\n",
    "    def process_text_and_generate_voice_array(slef,tts_fn, speaker, text, language, speed):\n",
    "        \"\"\"處理文本並生成聲音數據列表\"\"\"\n",
    "        text = text.replace('\\\\n', '').strip().replace('\\n', '')\n",
    "        symbol_remov = [\"，\", \"、\", \"。\", \"：\", \"）\", \"（\", \"？\", \":\", \")\", \"(\", \"「\", \"」\", \"！\"]\n",
    "        non_symbol_text = text\n",
    "        for symbol in symbol_remov:\n",
    "            non_symbol_text = non_symbol_text.replace(symbol, \"@\")\n",
    "\n",
    "        current_index = 0\n",
    "        max_len = 30\n",
    "        numpy_voice_array = []\n",
    "        if max_len >= len(text):\n",
    "            _, output = tts_fn(text=text, speaker=speaker, language=language, speed=float(speed))\n",
    "            numpy_voice_array = output[1].tolist()\n",
    "        else:\n",
    "            while True:\n",
    "                temp_text = non_symbol_text[current_index:current_index + max_len]\n",
    "                split_index = [pos for pos, char in enumerate(temp_text) if char == '@']\n",
    "                target_index = current_index + max_len + 1 if len(split_index) == 0 else split_index[-1] + current_index + 1\n",
    "                used_text = text[current_index:target_index]\n",
    "                _, output = tts_fn(text=used_text, speaker=speaker, language=language, speed=float(speed))\n",
    "                voice_list = output[1].tolist()\n",
    "                if len(voice_list) < 20000:\n",
    "                    numpy_voice_array += voice_list\n",
    "                else:\n",
    "                    numpy_voice_array += output[1].tolist()[300:-8500]\n",
    "                current_index = target_index\n",
    "                if current_index >= len(text) - 1:\n",
    "                    break\n",
    "\n",
    "        return numpy_voice_array, output[0]\n",
    "\n",
    "    def create_wav(self, text, filename=None, speaker = \"community\", language = \"中文\", speed=1.0):\n",
    "        \"\"\"根據文本創建wav檔案\"\"\"\n",
    "        numpy_voice_array, sr = self.process_text_and_generate_voice_array(self.tts_fn, speaker, text, language, speed)\n",
    "        numpy_voice_array2 = np.int16(np.array(numpy_voice_array) * 32767)\n",
    "       \n",
    "        # 構建完整的文件路徑\n",
    "        file_str = filename if filename else datetime.strftime(datetime.now(), '%Y-%m-%d-%H-%M-%S')\n",
    "        full_path = os.path.join('audio', f\"{file_str}.wav\")\n",
    "\n",
    "        # 確保目錄存在\n",
    "        os.makedirs(os.path.dirname(full_path), exist_ok=True)\n",
    "\n",
    "        # 寫入 WAV 文件\n",
    "        with wave.open(full_path, \"wb\") as wf:\n",
    "            wf.setnchannels(1)\n",
    "            wf.setsampwidth(2)\n",
    "            wf.setframerate(sr)\n",
    "            wf.writeframes(numpy_voice_array2.tobytes())\n",
    "        # 返回完整的文件路徑\n",
    "        return full_path\n",
    "\n",
    "    def create_voice_array(self, text, speaker = \"community\", language = \"中文\", speed=1.0):\n",
    "        \"\"\"根據文本生成聲音數據列表\"\"\"\n",
    "        numpy_voice_array, _ = self.process_text_and_generate_voice_array(self.tts_fn, speaker, text, language, speed)\n",
    "        return numpy_voice_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tts = VitsService(\n",
    "    hparams_file_path=\"./models/YunzeNeural/config.json\",\n",
    "    checkpoint_path=\"./models/YunzeNeural/G_latest.pth\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "speaker = 'YunzeNeural'\n",
    "tts.create_wav(text='水藥，須冷藏保存，若無法冷藏，請放置陰涼處',speaker=speaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
